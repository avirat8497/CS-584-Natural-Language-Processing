{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a function to Remove Punctuation and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(n):\n",
    "    return ''.join(x for x in n if x not in set(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"train.txt\", encoding = \"utf-8\")\n",
    "file1 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1_cleaned = remove_punctuation(re.sub(\"\\n\",\"\", file1)).lower()\n",
    "tokens = file1_cleaned.split()\n",
    "train_one_tokens = Counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tokens for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = len(train_one_tokens)\n",
    "keys = list(train_one_tokens.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframe or Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(np.zeros(cells*cells).reshape(cells,cells), columns=keys, index=keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_gt = pd.DataFrame(np.zeros(cells*cells).reshape(cells,cells), columns=keys, index=keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_kn = pd.DataFrame(np.zeros(cells*cells).reshape(cells,cells), columns=keys, index=keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aer</th>\n",
       "      <th>banknote</th>\n",
       "      <th>berlitz</th>\n",
       "      <th>calloway</th>\n",
       "      <th>centrust</th>\n",
       "      <th>cluett</th>\n",
       "      <th>fromstein</th>\n",
       "      <th>gitano</th>\n",
       "      <th>guterman</th>\n",
       "      <th>hydroquebec</th>\n",
       "      <th>...</th>\n",
       "      <th>lungcancer</th>\n",
       "      <th>bikers</th>\n",
       "      <th>bofors</th>\n",
       "      <th>parsow</th>\n",
       "      <th>caci</th>\n",
       "      <th>isi</th>\n",
       "      <th>chestman</th>\n",
       "      <th>tci</th>\n",
       "      <th>trecker</th>\n",
       "      <th>unilab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aer</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banknote</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>berlitz</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calloway</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>centrust</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9930 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          aer  banknote  berlitz  calloway  centrust  cluett  fromstein  \\\n",
       "aer       0.0       0.0      0.0       0.0       0.0     0.0        0.0   \n",
       "banknote  0.0       0.0      0.0       0.0       0.0     0.0        0.0   \n",
       "berlitz   0.0       0.0      0.0       0.0       0.0     0.0        0.0   \n",
       "calloway  0.0       0.0      0.0       0.0       0.0     0.0        0.0   \n",
       "centrust  0.0       0.0      0.0       0.0       0.0     0.0        0.0   \n",
       "\n",
       "          gitano  guterman  hydroquebec  ...  lungcancer  bikers  bofors  \\\n",
       "aer          0.0       0.0          0.0  ...         0.0     0.0     0.0   \n",
       "banknote     0.0       0.0          0.0  ...         0.0     0.0     0.0   \n",
       "berlitz      0.0       0.0          0.0  ...         0.0     0.0     0.0   \n",
       "calloway     0.0       0.0          0.0  ...         0.0     0.0     0.0   \n",
       "centrust     0.0       0.0          0.0  ...         0.0     0.0     0.0   \n",
       "\n",
       "          parsow  caci  isi  chestman  tci  trecker  unilab  \n",
       "aer          0.0   0.0  0.0       0.0  0.0      0.0     0.0  \n",
       "banknote     0.0   0.0  0.0       0.0  0.0      0.0     0.0  \n",
       "berlitz      0.0   0.0  0.0       0.0  0.0      0.0     0.0  \n",
       "calloway     0.0   0.0  0.0       0.0  0.0      0.0     0.0  \n",
       "centrust     0.0   0.0  0.0       0.0  0.0      0.0     0.0  \n",
       "\n",
       "[5 rows x 9930 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Ngram Model (Bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigr_list = nltk.bigrams(file1_cleaned.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = []\n",
    "for bigram in bigr_list:\n",
    "    bi.append(bigram)\n",
    "bi_dict = Counter(bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(keys, bigram_dict, one_tokens, df):\n",
    "    #Getting the probability of each key\n",
    "    for key1 in keys:\n",
    "        for key2 in keys:\n",
    "            if bigram_dict[(key1,key2)] > 0 and one_tokens[key1] > 0:\n",
    "                key_prob = bigram_dict[(key1,key2)] / one_tokens[key1]\n",
    "                dataset.at[key1,key2] = key_prob \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Function for predicting words using Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(dataset, keys):\n",
    "    highest_prob = list(dataset.idxmax(axis=1))\n",
    "        \n",
    "        #Mapping keys to highest prob\t\n",
    "    predicted_values = defaultdict() \n",
    "    for i in range(len(highest_prob)):\n",
    "        predicted_values.update({keys[i]:highest_prob[i]})\n",
    "        \n",
    "    return predicted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create function to print Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def print_predictions(predicted_values, name):\n",
    "    for k, v in predicted_values.items():\n",
    "            filename = f\"predicted_{name}.txt\"\n",
    "        #if v  not in 'aer':\n",
    "            f = open(filename,'a')\n",
    "            f.write(f\"{k} : {v}\\n\")\n",
    "            #print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ngram(keys, bi_dict, train_one_tokens, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14018691588785046"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.at['floor','traders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values_us = predictions(dataset, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_predictions(predicted_values_us, \"ngram\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Good turing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_turing(keys, bigram_dict, df):\n",
    "    n_c = Counter(list(bigram_dict.values()))\n",
    "    #print(n_c)\n",
    "    length_corpus = len(bi)\n",
    "    default_val = n_c[1] / length_corpus\n",
    "    #print(default_val)\n",
    "    \n",
    "    for key1 in keys:\n",
    "            for key2 in keys:\n",
    "                og_count = bigram_dict[(key1,key2)]\n",
    "                if og_count == 0:\n",
    "                    dataset.at[key1, key2] = default_val\n",
    "                else:\n",
    "                    adjusted_count = ((og_count+1) * n_c[og_count+1]) / n_c[og_count]\n",
    "                    gt_prob = adjusted_count / total_corpus\n",
    "                    dataset.at[key1,key2] = gt_prob\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 189376, 2: 36725, 3: 15425, 4: 8433, 5: 5376, 6: 3583, 7: 2711, 8: 1923, 9: 1520, 10: 1287, 11: 1103, 12: 844, 13: 712, 14: 626, 15: 495, 16: 470, 17: 389, 18: 323, 19: 307, 20: 257, 22: 255, 21: 246, 23: 223, 24: 202, 26: 174, 25: 171, 27: 145, 28: 141, 29: 124, 30: 119, 32: 94, 31: 94, 34: 87, 33: 80, 35: 71, 36: 69, 38: 64, 41: 58, 40: 56, 42: 54, 39: 52, 43: 52, 37: 51, 47: 50, 46: 47, 44: 45, 45: 45, 49: 44, 51: 39, 50: 34, 54: 34, 56: 34, 52: 33, 59: 31, 48: 30, 55: 25, 60: 25, 65: 24, 58: 24, 74: 23, 72: 23, 62: 22, 57: 22, 70: 21, 63: 21, 64: 21, 71: 19, 81: 19, 68: 18, 53: 18, 67: 18, 76: 17, 73: 16, 77: 16, 69: 16, 100: 15, 66: 15, 82: 14, 80: 13, 79: 13, 89: 12, 75: 12, 91: 12, 85: 12, 61: 11, 78: 11, 127: 10, 92: 10, 86: 10, 84: 9, 95: 9, 120: 9, 83: 9, 97: 8, 112: 8, 88: 8, 161: 8, 101: 8, 184: 7, 94: 7, 131: 7, 96: 7, 148: 6, 99: 6, 143: 6, 132: 6, 115: 6, 111: 6, 103: 6, 98: 6, 113: 6, 197: 5, 128: 5, 107: 5, 104: 5, 93: 5, 87: 5, 135: 5, 144: 5, 106: 5, 105: 5, 109: 4, 122: 4, 178: 4, 188: 4, 124: 4, 156: 4, 146: 4, 222: 4, 230: 4, 138: 4, 187: 4, 164: 4, 224: 4, 117: 4, 118: 4, 110: 4, 90: 4, 177: 4, 160: 3, 194: 3, 179: 3, 162: 3, 116: 3, 195: 3, 145: 3, 175: 3, 266: 3, 159: 3, 155: 3, 172: 3, 121: 3, 139: 3, 114: 3, 235: 3, 264: 3, 180: 3, 142: 3, 267: 3, 102: 3, 123: 3, 249: 3, 149: 3, 193: 3, 151: 3, 168: 3, 259: 2, 403: 2, 198: 2, 252: 2, 108: 2, 199: 2, 255: 2, 395: 2, 207: 2, 176: 2, 955: 2, 505: 2, 310: 2, 351: 2, 299: 2, 275: 2, 283: 2, 238: 2, 352: 2, 428: 2, 174: 2, 327: 2, 186: 2, 169: 2, 402: 2, 219: 2, 182: 2, 214: 2, 137: 2, 226: 2, 319: 2, 154: 2, 150: 2, 204: 2, 201: 2, 202: 2, 210: 2, 167: 2, 130: 2, 296: 2, 119: 2, 126: 2, 206: 2, 134: 2, 279: 2, 246: 2, 221: 2, 152: 2, 181: 2, 140: 2, 129: 2, 165: 2, 223: 2, 190: 2, 141: 2, 379: 1, 370: 1, 670: 1, 1112: 1, 1799: 1, 309: 1, 697: 1, 470: 1, 192: 1, 3876: 1, 439: 1, 1190: 1, 978: 1, 652: 1, 1738: 1, 2523: 1, 4225: 1, 475: 1, 237: 1, 5293: 1, 1599: 1, 911: 1, 1139: 1, 1941: 1, 4507: 1, 356: 1, 1144: 1, 2059: 1, 2865: 1, 7204: 1, 1851: 1, 929: 1, 1647: 1, 1892: 1, 1826: 1, 218: 1, 1058: 1, 549: 1, 1979: 1, 983: 1, 1115: 1, 543: 1, 943: 1, 1244: 1, 907: 1, 342: 1, 423: 1, 368: 1, 445: 1, 441: 1, 1223: 1, 1036: 1, 302: 1, 2018: 1, 944: 1, 821: 1, 1030: 1, 433: 1, 1046: 1, 269: 1, 320: 1, 270: 1, 245: 1, 254: 1, 823: 1, 495: 1, 957: 1, 217: 1, 216: 1, 603: 1, 482: 1, 277: 1, 504: 1, 421: 1, 294: 1, 928: 1, 808: 1, 986: 1, 125: 1, 380: 1, 1120: 1, 797: 1, 4493: 1, 969: 1, 213: 1, 203: 1, 645: 1, 780: 1, 268: 1, 263: 1, 354: 1, 191: 1, 456: 1, 677: 1, 273: 1, 635: 1, 485: 1, 1713: 1, 371: 1, 387: 1, 400: 1, 583: 1, 432: 1, 328: 1, 393: 1, 381: 1, 708: 1, 314: 1, 397: 1, 755: 1, 514: 1, 576: 1, 468: 1, 250: 1, 276: 1, 304: 1, 564: 1, 401: 1, 454: 1, 729: 1, 946: 1, 346: 1, 528: 1, 398: 1, 633: 1, 887: 1, 688: 1, 347: 1, 474: 1, 592: 1, 345: 1, 463: 1, 508: 1, 288: 1, 243: 1, 232: 1, 153: 1, 166: 1, 305: 1, 341: 1, 278: 1, 285: 1, 286: 1, 355: 1, 330: 1, 301: 1, 133: 1, 265: 1, 228: 1, 227: 1, 229: 1, 425: 1, 208: 1, 170: 1, 147: 1, 365: 1, 233: 1, 234: 1, 284: 1, 358: 1, 239: 1, 251: 1, 511: 1, 287: 1, 157: 1, 173: 1, 205: 1, 183: 1})\n",
      "0.2156693258625304\n"
     ]
    }
   ],
   "source": [
    "dataset_gt = good_turing(keys, bigram_dict, dataset_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aer</th>\n",
       "      <th>banknote</th>\n",
       "      <th>berlitz</th>\n",
       "      <th>calloway</th>\n",
       "      <th>centrust</th>\n",
       "      <th>cluett</th>\n",
       "      <th>fromstein</th>\n",
       "      <th>gitano</th>\n",
       "      <th>guterman</th>\n",
       "      <th>hydroquebec</th>\n",
       "      <th>...</th>\n",
       "      <th>lungcancer</th>\n",
       "      <th>bikers</th>\n",
       "      <th>bofors</th>\n",
       "      <th>parsow</th>\n",
       "      <th>caci</th>\n",
       "      <th>isi</th>\n",
       "      <th>chestman</th>\n",
       "      <th>tci</th>\n",
       "      <th>trecker</th>\n",
       "      <th>unilab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>aer</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>4.417030e-07</td>\n",
       "      <td>2.156693e-01</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>banknote</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>2.156693e-01</td>\n",
       "      <td>4.417030e-07</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.215669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 9930 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               aer      banknote       berlitz  calloway  centrust    cluett  \\\n",
       "aer       0.215669  4.417030e-07  2.156693e-01  0.215669  0.215669  0.215669   \n",
       "banknote  0.215669  2.156693e-01  4.417030e-07  0.215669  0.215669  0.215669   \n",
       "\n",
       "          fromstein    gitano  guterman  hydroquebec  ...  lungcancer  \\\n",
       "aer        0.215669  0.215669  0.215669     0.215669  ...    0.215669   \n",
       "banknote   0.215669  0.215669  0.215669     0.215669  ...    0.215669   \n",
       "\n",
       "            bikers    bofors    parsow      caci       isi  chestman  \\\n",
       "aer       0.215669  0.215669  0.215669  0.215669  0.215669  0.215669   \n",
       "banknote  0.215669  0.215669  0.215669  0.215669  0.215669  0.215669   \n",
       "\n",
       "               tci   trecker    unilab  \n",
       "aer       0.215669  0.215669  0.215669  \n",
       "banknote  0.215669  0.215669  0.215669  \n",
       "\n",
       "[2 rows x 9930 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_gt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values_gt = predictions(dataset_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions using Good Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(predicted_values)\n",
    "    for k, v in predicted_values.items():\n",
    "            f = open('predicted_gt.txt','a')\n",
    "            f.write(f\"{k} : {v}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on valid.txt to check perplexity of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_file = open(\"valid.txt\", encoding = \"utf-8\")\n",
    "file1_valid_data = valid_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1_valid_data_cleaned = remove_punctuation(re.sub(\"\\n\",\"\", file1_valid_data)).lower()\n",
    "valid_tokens = file1_valid_data_cleaned.split()\n",
    "valid_one_tokens = Counter(valid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_cells = len(valid_one_tokens)\n",
    "valid_keys = list(valid_one_tokens.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(valid_keys)\n",
    "dataset_valid = pd.DataFrame(np.zeros(valid_cells*valid_cells).reshape(valid_cells,valid_cells), columns=valid_keys, index=valid_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consumers</th>\n",
       "      <th>may</th>\n",
       "      <th>want</th>\n",
       "      <th>to</th>\n",
       "      <th>move</th>\n",
       "      <th>their</th>\n",
       "      <th>telephones</th>\n",
       "      <th>a</th>\n",
       "      <th>little</th>\n",
       "      <th>closer</th>\n",
       "      <th>...</th>\n",
       "      <th>tools</th>\n",
       "      <th>45yearold</th>\n",
       "      <th>spurring</th>\n",
       "      <th>milwaukee</th>\n",
       "      <th>reinvestment</th>\n",
       "      <th>controlling</th>\n",
       "      <th>satisfactory</th>\n",
       "      <th>rewards</th>\n",
       "      <th>patience</th>\n",
       "      <th>driver</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>consumers</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 5979 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           consumers  may  want   to  move  their  telephones    a  little  \\\n",
       "consumers        0.0  0.0   0.0  0.0   0.0    0.0         0.0  0.0     0.0   \n",
       "may              0.0  0.0   0.0  0.0   0.0    0.0         0.0  0.0     0.0   \n",
       "want             0.0  0.0   0.0  0.0   0.0    0.0         0.0  0.0     0.0   \n",
       "\n",
       "           closer  ...  tools  45yearold  spurring  milwaukee  reinvestment  \\\n",
       "consumers     0.0  ...    0.0        0.0       0.0        0.0           0.0   \n",
       "may           0.0  ...    0.0        0.0       0.0        0.0           0.0   \n",
       "want          0.0  ...    0.0        0.0       0.0        0.0           0.0   \n",
       "\n",
       "           controlling  satisfactory  rewards  patience  driver  \n",
       "consumers          0.0           0.0      0.0       0.0     0.0  \n",
       "may                0.0           0.0      0.0       0.0     0.0  \n",
       "want               0.0           0.0      0.0       0.0     0.0  \n",
       "\n",
       "[3 rows x 5979 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_valid.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_bi_list = nltk.bigrams(file1_valid_data_cleaned.split())\n",
    "valid_bigram = []\n",
    "for bigram in valid_bi_list:\n",
    "    valid_bigram.append(bigram)\n",
    "valid_bi_dict = Counter(valid_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = ngram(valid_keys, valid_bi_dict, valid_one_tokens, dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values_test = predictions(dataset_valid, valid_keys)\n",
    "print_predictions(predicted_values_test,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_predictions(predicted_values):\n",
    "    predict = open('input.txt','r')\n",
    "    lines = predict.readlines()\n",
    "    input_test = lines[:30]\n",
    "    for line in input_test:\n",
    "        predicted_words_list = []\n",
    "        line = re.sub(\"___\", \"\",line.rstrip())\n",
    "        line_list = line.split()\n",
    "        last_word = line_list[-1]\n",
    "        while predicted_values[last_word] is not None:\n",
    "            pred = predicted_values[last_word]\n",
    "            if pred == 'unk' or pred == 'n': break\n",
    "            #print(pred)\n",
    "            predicted_words_list.append(pred)\n",
    "            last_word = pred\n",
    "        \n",
    "        print(f\"{line}->{predicted_words_list}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but while the new york stock exchange did n't fall ->['in', 'the']\n",
      "some circuit breakers installed after the october N crash failed ->['to']\n",
      "the N stock specialist firms on the big board floor ->['traders', 'said', 'the']\n",
      "big investment banks refused to step up to the plate ->['and']\n",
      "heavy selling of standard & poor 's 500-stock index futures ->['prices', 'for', 'the']\n",
      "seven big board stocks ual amr bankamerica walt disney capital ->['markets', 'in', 'the']\n",
      "once again the specialists were not able to handle the ->[]\n",
      "<unk> james <unk> chairman of specialists henderson brothers inc. it ->['s']\n",
      "when the dollar is in a <unk> even central banks ->['and']\n",
      "speculators are calling for a degree of liquidity that is ->['a']\n",
      "many money managers and some traders had already left their ->[]\n",
      "then in a <unk> plunge the dow jones industrials in ->['the']\n",
      "<unk> trading accelerated to N million shares a record for ->['the']\n",
      "at the end of the day N million shares were ->[]\n",
      "the dow 's decline was second in point terms only ->[]\n",
      "in percentage terms however the dow 's dive was the ->[]\n",
      "shares of ual the parent of united airlines were extremely ->['high', 'as', 'a']\n",
      "wall street 's takeover-stock speculators or risk arbitragers had placed ->['directly', 'comparable', 'each']\n",
      "at N p.m. edt came the <unk> news the big ->['board', 'of', 'the']\n",
      "on the exchange floor as soon as ual stopped trading ->['on', 'the']\n",
      "several traders could be seen shaking their heads when the ->[]\n",
      "for weeks the market had been nervous about takeovers after ->['the']\n",
      "and N minutes after the ual trading halt came news ->['conference', 'on', 'the']\n",
      "arbitragers could n't dump their ual stock but they rid ->['of', 'the']\n",
      "for example their selling caused trading halts to be declared ->['a']\n",
      "but as panic spread speculators began to sell blue-chip stocks ->['in', 'the']\n",
      "when trading was halted in philip morris the stock was ->[]\n",
      "selling <unk> because of waves of automatic stop-loss orders which ->['is', 'a']\n",
      "most of the stock selling pressure came from wall street ->['journal', 's']\n",
      "traders said most of their major institutional investors on the ->[]\n"
     ]
    }
   ],
   "source": [
    "ngram_predictions(predicted_values_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nesser Function to predict words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nesser():\n",
    "    d = 0.04\n",
    "    for key1 in valid_keys:\n",
    "                for key2 in valid_keys:\n",
    "                    og_count = valid_bi_dict[(key1,key2)] \n",
    "                    single_token_count = valid_single_tokens[key2]\n",
    "                    kn_prob = max((og_count - d), 0) / (d / single_token_count )\n",
    "                    df.at[key1, key2] = kn_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Perplexity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06853272882625158"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = 0\n",
    "for bigram in valid_bigram:\n",
    "    key1, key2 = bigram\n",
    "    if df.at[key1, key2] > 0:\n",
    "            prob = prob + np.log(dataset_test.at[key1, key2])\n",
    "ppr = np.exp(prob/ len(valid_bigram))\n",
    "ppr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
